18:11:21.720 Training @ 0 epoch...
18:11:21.973   Training iter 50, batch loss 2.1211, batch acc 0.3466
18:11:22.211   Training iter 100, batch loss 1.1001, batch acc 0.7314
18:11:22.423   Training iter 150, batch loss 0.5847, batch acc 0.8522
18:11:22.613   Training iter 200, batch loss 0.4920, batch acc 0.8564
18:11:22.870   Training iter 250, batch loss 0.4338, batch acc 0.8784
18:11:23.069   Training iter 300, batch loss 0.3895, batch acc 0.8926
18:11:23.223   Training iter 350, batch loss 0.3818, batch acc 0.8912
18:11:23.423   Training iter 400, batch loss 0.3656, batch acc 0.8952
18:11:23.608   Training iter 450, batch loss 0.3514, batch acc 0.8960
18:11:23.810   Training iter 500, batch loss 0.3361, batch acc 0.9040
18:11:24.056   Training iter 550, batch loss 0.3137, batch acc 0.9090
18:11:24.212   Training iter 600, batch loss 0.3205, batch acc 0.9044
18:11:24.212 Testing @ 0 epoch...
18:11:24.319     Testing, total mean loss 0.31638, total acc 0.90760
18:11:24.319 Training @ 1 epoch...
18:11:24.408   Training iter 50, batch loss 0.3063, batch acc 0.9178
18:11:24.558   Training iter 100, batch loss 0.2863, batch acc 0.9152
18:11:24.674   Training iter 150, batch loss 0.3268, batch acc 0.9060
18:11:24.810   Training iter 200, batch loss 0.3043, batch acc 0.9120
18:11:24.926   Training iter 250, batch loss 0.3195, batch acc 0.9058
18:11:25.140   Training iter 300, batch loss 0.3107, batch acc 0.9078
18:11:25.324   Training iter 350, batch loss 0.3061, batch acc 0.9120
18:11:25.469   Training iter 400, batch loss 0.3056, batch acc 0.9136
18:11:25.610   Training iter 450, batch loss 0.3123, batch acc 0.9110
18:11:25.857   Training iter 500, batch loss 0.2827, batch acc 0.9176
18:11:26.043   Training iter 550, batch loss 0.2832, batch acc 0.9126
18:11:26.251   Training iter 600, batch loss 0.2940, batch acc 0.9134
18:11:26.252 Training @ 2 epoch...
18:11:26.403   Training iter 50, batch loss 0.2650, batch acc 0.9202
18:11:26.578   Training iter 100, batch loss 0.2845, batch acc 0.9188
18:11:26.738   Training iter 150, batch loss 0.2716, batch acc 0.9236
18:11:26.856   Training iter 200, batch loss 0.2825, batch acc 0.9168
18:11:26.979   Training iter 250, batch loss 0.2985, batch acc 0.9104
18:11:27.146   Training iter 300, batch loss 0.2768, batch acc 0.9154
18:11:27.278   Training iter 350, batch loss 0.2640, batch acc 0.9220
18:11:27.411   Training iter 400, batch loss 0.2747, batch acc 0.9190
18:11:27.529   Training iter 450, batch loss 0.2432, batch acc 0.9308
18:11:27.679   Training iter 500, batch loss 0.2671, batch acc 0.9268
18:11:27.803   Training iter 550, batch loss 0.2563, batch acc 0.9222
18:11:27.935   Training iter 600, batch loss 0.2647, batch acc 0.9266
18:11:27.936 Training @ 3 epoch...
18:11:28.525   Training iter 50, batch loss 0.2351, batch acc 0.9302
18:11:28.717   Training iter 100, batch loss 0.2494, batch acc 0.9268
18:11:29.100   Training iter 150, batch loss 0.2544, batch acc 0.9276
18:11:29.388   Training iter 200, batch loss 0.2564, batch acc 0.9254
18:11:29.545   Training iter 250, batch loss 0.2502, batch acc 0.9344
18:11:29.727   Training iter 300, batch loss 0.2365, batch acc 0.9320
18:11:29.838   Training iter 350, batch loss 0.2354, batch acc 0.9300
18:11:30.274   Training iter 400, batch loss 0.2546, batch acc 0.9258
18:11:30.668   Training iter 450, batch loss 0.2338, batch acc 0.9306
18:11:30.772   Training iter 500, batch loss 0.2348, batch acc 0.9300
18:11:30.896   Training iter 550, batch loss 0.2477, batch acc 0.9264
18:11:31.011   Training iter 600, batch loss 0.2066, batch acc 0.9394
18:11:31.012 Training @ 4 epoch...
18:11:31.102   Training iter 50, batch loss 0.2261, batch acc 0.9366
18:11:31.244   Training iter 100, batch loss 0.2314, batch acc 0.9314
18:11:31.353   Training iter 150, batch loss 0.2117, batch acc 0.9394
18:11:31.448   Training iter 200, batch loss 0.2153, batch acc 0.9398
18:11:31.561   Training iter 250, batch loss 0.2048, batch acc 0.9448
18:11:31.692   Training iter 300, batch loss 0.1981, batch acc 0.9410
18:11:31.816   Training iter 350, batch loss 0.2061, batch acc 0.9382
18:11:31.946   Training iter 400, batch loss 0.2089, batch acc 0.9378
18:11:32.043   Training iter 450, batch loss 0.2026, batch acc 0.9406
18:11:32.160   Training iter 500, batch loss 0.2023, batch acc 0.9394
18:11:32.276   Training iter 550, batch loss 0.2144, batch acc 0.9394
18:11:32.414   Training iter 600, batch loss 0.2030, batch acc 0.9430
18:11:32.415 Training @ 5 epoch...
18:11:32.549   Training iter 50, batch loss 0.1842, batch acc 0.9492
18:11:32.721   Training iter 100, batch loss 0.1779, batch acc 0.9496
18:11:32.830   Training iter 150, batch loss 0.1815, batch acc 0.9498
18:11:32.944   Training iter 200, batch loss 0.1759, batch acc 0.9494
18:11:33.043   Training iter 250, batch loss 0.2048, batch acc 0.9418
18:11:33.184   Training iter 300, batch loss 0.1798, batch acc 0.9470
18:11:33.321   Training iter 350, batch loss 0.1908, batch acc 0.9470
18:11:33.588   Training iter 400, batch loss 0.1918, batch acc 0.9452
18:11:33.687   Training iter 450, batch loss 0.2035, batch acc 0.9370
18:11:33.784   Training iter 500, batch loss 0.1908, batch acc 0.9462
18:11:33.875   Training iter 550, batch loss 0.1856, batch acc 0.9436
18:11:33.959   Training iter 600, batch loss 0.1753, batch acc 0.9498
18:11:33.960 Testing @ 5 epoch...
18:11:34.049     Testing, total mean loss 0.19657, total acc 0.94130
18:11:34.049 Training @ 6 epoch...
18:11:34.144   Training iter 50, batch loss 0.1784, batch acc 0.9466
18:11:34.256   Training iter 100, batch loss 0.1726, batch acc 0.9524
18:11:34.348   Training iter 150, batch loss 0.1783, batch acc 0.9454
18:11:34.452   Training iter 200, batch loss 0.1709, batch acc 0.9510
18:11:34.553   Training iter 250, batch loss 0.1761, batch acc 0.9500
18:11:34.673   Training iter 300, batch loss 0.1649, batch acc 0.9528
18:11:34.782   Training iter 350, batch loss 0.1710, batch acc 0.9504
18:11:34.987   Training iter 400, batch loss 0.1554, batch acc 0.9516
18:11:35.342   Training iter 450, batch loss 0.1605, batch acc 0.9550
18:11:35.660   Training iter 500, batch loss 0.1601, batch acc 0.9576
18:11:35.820   Training iter 550, batch loss 0.1759, batch acc 0.9514
18:11:35.921   Training iter 600, batch loss 0.1672, batch acc 0.9532
18:11:35.922 Training @ 7 epoch...
18:11:36.018   Training iter 50, batch loss 0.1619, batch acc 0.9528
18:11:36.258   Training iter 100, batch loss 0.1388, batch acc 0.9592
18:11:36.430   Training iter 150, batch loss 0.1607, batch acc 0.9538
18:11:36.611   Training iter 200, batch loss 0.1683, batch acc 0.9508
18:11:36.781   Training iter 250, batch loss 0.1536, batch acc 0.9556
18:11:36.950   Training iter 300, batch loss 0.1581, batch acc 0.9578
18:11:37.258   Training iter 350, batch loss 0.1533, batch acc 0.9604
18:11:37.365   Training iter 400, batch loss 0.1568, batch acc 0.9578
18:11:37.486   Training iter 450, batch loss 0.1353, batch acc 0.9610
18:11:37.563   Training iter 500, batch loss 0.1405, batch acc 0.9612
18:11:37.743   Training iter 550, batch loss 0.1542, batch acc 0.9548
18:11:37.872   Training iter 600, batch loss 0.1540, batch acc 0.9530
18:11:37.873 Training @ 8 epoch...
18:11:38.039   Training iter 50, batch loss 0.1386, batch acc 0.9610
18:11:38.216   Training iter 100, batch loss 0.1389, batch acc 0.9606
18:11:38.356   Training iter 150, batch loss 0.1228, batch acc 0.9636
18:11:38.475   Training iter 200, batch loss 0.1547, batch acc 0.9552
18:11:38.650   Training iter 250, batch loss 0.1359, batch acc 0.9610
18:11:38.769   Training iter 300, batch loss 0.1315, batch acc 0.9626
18:11:38.882   Training iter 350, batch loss 0.1468, batch acc 0.9574
18:11:39.191   Training iter 400, batch loss 0.1496, batch acc 0.9586
18:11:39.458   Training iter 450, batch loss 0.1375, batch acc 0.9600
18:11:39.573   Training iter 500, batch loss 0.1450, batch acc 0.9572
18:11:39.907   Training iter 550, batch loss 0.1714, batch acc 0.9486
18:11:40.127   Training iter 600, batch loss 0.1332, batch acc 0.9610
18:11:40.129 Training @ 9 epoch...
18:11:40.295   Training iter 50, batch loss 0.1279, batch acc 0.9618
18:11:40.438   Training iter 100, batch loss 0.1327, batch acc 0.9658
18:11:40.556   Training iter 150, batch loss 0.1316, batch acc 0.9630
18:11:40.693   Training iter 200, batch loss 0.1275, batch acc 0.9662
18:11:40.848   Training iter 250, batch loss 0.1311, batch acc 0.9634
18:11:41.010   Training iter 300, batch loss 0.1348, batch acc 0.9602
18:11:41.132   Training iter 350, batch loss 0.1413, batch acc 0.9580
18:11:41.245   Training iter 400, batch loss 0.1212, batch acc 0.9628
18:11:41.357   Training iter 450, batch loss 0.1258, batch acc 0.9638
18:11:41.450   Training iter 500, batch loss 0.1392, batch acc 0.9600
18:11:41.556   Training iter 550, batch loss 0.1259, batch acc 0.9670
18:11:41.661   Training iter 600, batch loss 0.1292, batch acc 0.9614
18:11:41.661 Training @ 10 epoch...
18:11:41.752   Training iter 50, batch loss 0.1200, batch acc 0.9660
18:11:41.852   Training iter 100, batch loss 0.1340, batch acc 0.9648
18:11:41.963   Training iter 150, batch loss 0.1130, batch acc 0.9678
18:11:42.123   Training iter 200, batch loss 0.1266, batch acc 0.9636
18:11:42.324   Training iter 250, batch loss 0.1297, batch acc 0.9638
18:11:42.599   Training iter 300, batch loss 0.1164, batch acc 0.9650
18:11:42.910   Training iter 350, batch loss 0.1072, batch acc 0.9704
18:11:43.219   Training iter 400, batch loss 0.1390, batch acc 0.9606
18:11:43.495   Training iter 450, batch loss 0.1230, batch acc 0.9672
18:11:43.712   Training iter 500, batch loss 0.1190, batch acc 0.9664
18:11:43.854   Training iter 550, batch loss 0.1293, batch acc 0.9628
18:11:43.991   Training iter 600, batch loss 0.1232, batch acc 0.9644
18:11:43.992 Testing @ 10 epoch...
18:11:44.203     Testing, total mean loss 0.12626, total acc 0.96170
18:11:44.203 Training @ 11 epoch...
18:11:44.545   Training iter 50, batch loss 0.1111, batch acc 0.9686
18:11:44.759   Training iter 100, batch loss 0.1211, batch acc 0.9682
18:11:44.861   Training iter 150, batch loss 0.1092, batch acc 0.9676
18:11:44.966   Training iter 200, batch loss 0.1188, batch acc 0.9664
18:11:45.062   Training iter 250, batch loss 0.1138, batch acc 0.9690
18:11:45.162   Training iter 300, batch loss 0.1153, batch acc 0.9670
18:11:45.261   Training iter 350, batch loss 0.1229, batch acc 0.9646
18:11:45.365   Training iter 400, batch loss 0.1104, batch acc 0.9666
18:11:45.478   Training iter 450, batch loss 0.1070, batch acc 0.9686
18:11:45.575   Training iter 500, batch loss 0.1124, batch acc 0.9688
18:11:45.667   Training iter 550, batch loss 0.1172, batch acc 0.9680
18:11:45.817   Training iter 600, batch loss 0.1287, batch acc 0.9648
18:11:45.819 Training @ 12 epoch...
18:11:45.980   Training iter 50, batch loss 0.1138, batch acc 0.9680
18:11:46.130   Training iter 100, batch loss 0.1096, batch acc 0.9694
18:11:46.289   Training iter 150, batch loss 0.1205, batch acc 0.9644
18:11:46.561   Training iter 200, batch loss 0.1128, batch acc 0.9684
18:11:46.737   Training iter 250, batch loss 0.1116, batch acc 0.9696
18:11:46.960   Training iter 300, batch loss 0.1080, batch acc 0.9710
18:11:47.145   Training iter 350, batch loss 0.1075, batch acc 0.9686
18:11:47.497   Training iter 400, batch loss 0.1099, batch acc 0.9678
18:11:47.753   Training iter 450, batch loss 0.1072, batch acc 0.9706
18:11:47.993   Training iter 500, batch loss 0.0976, batch acc 0.9710
18:11:48.288   Training iter 550, batch loss 0.1050, batch acc 0.9700
18:11:48.476   Training iter 600, batch loss 0.1085, batch acc 0.9702
18:11:48.478 Training @ 13 epoch...
18:11:48.594   Training iter 50, batch loss 0.1155, batch acc 0.9668
18:11:48.729   Training iter 100, batch loss 0.1069, batch acc 0.9680
18:11:48.863   Training iter 150, batch loss 0.1183, batch acc 0.9644
18:11:49.128   Training iter 200, batch loss 0.0965, batch acc 0.9714
18:11:49.270   Training iter 250, batch loss 0.1002, batch acc 0.9710
18:11:49.439   Training iter 300, batch loss 0.1107, batch acc 0.9692
18:11:49.664   Training iter 350, batch loss 0.0969, batch acc 0.9704
18:11:50.190   Training iter 400, batch loss 0.1042, batch acc 0.9714
18:11:50.394   Training iter 450, batch loss 0.1079, batch acc 0.9694
18:11:50.677   Training iter 500, batch loss 0.1026, batch acc 0.9704
18:11:51.337   Training iter 550, batch loss 0.1014, batch acc 0.9716
18:11:51.748   Training iter 600, batch loss 0.0977, batch acc 0.9726
18:11:51.749 Training @ 14 epoch...
18:11:51.989   Training iter 50, batch loss 0.0973, batch acc 0.9708
18:11:52.169   Training iter 100, batch loss 0.0943, batch acc 0.9722
18:11:52.412   Training iter 150, batch loss 0.1124, batch acc 0.9678
18:11:52.758   Training iter 200, batch loss 0.0982, batch acc 0.9722
Traceback (most recent call last):
  File "/Users/wangjuanli/Codefield/2023Fall/ANN/HW1/codes/run_mlp.py", line 84, in <module>
    train_net(model, loss, config, train_data, train_label, config.batch_size, config.disp_freq, epoch, config.name)
  File "/Users/wangjuanli/Codefield/2023Fall/ANN/HW1/codes/solve_net.py", line 31, in train_net
    loss_value = loss.forward(output, target)
  File "/Users/wangjuanli/Codefield/2023Fall/ANN/HW1/codes/loss.py", line 26, in forward
    h = np.exp(input) / np.sum(np.exp(input), axis=1, keepdims=True)
KeyboardInterrupt